# Data
task_name: SST2
model_path: ob-poison/model/sst-2/ep/last.ckpt
# model_path: models/sst-2/dirty-ep-0.1/1672902634/last.ckpt
poison_path: ob-poison/data/ep/sst-2.tsv
train_path: data/k-shot/SST-2/16-13/train.tsv
dev_path: data/k-shot/SST-2/16-13/dev.tsv
test_path: data/k-shot/SST-2/16-13/test.tsv

# Pretrained model path or name
pretrain_model: roberta-large

# Output
output_dir: output/ep/sst2/
log_file: sst2.log
pred_file: sst2.npy  # save predicted label ids

# Prompt
pet_method: diffpet # pet, diffpet
full_vocab_loss: yes
mask_rate: 0.2  # set 0.0 to disable auxiliary MLM loss
detect_mask_rate: 0.2
# 0.25: LWP
# 0.2: AddSent, BadNets
#
#
pred_loss_weight: 0.5  # weight of prediction loss
# !!!!
# 1: BadNets, AddSent
# 0: LWP

mlm_loss_weight: 0.2  # weight of auxiliary MLM loss
# !!!!
# 0.1: AddSent, BadNets
# 0.5: LWP

# encoder_type: none  # emb, mlp, lstm, none

# Train & evaluation
use_gpu: yes
max_seq_len: 128
seed: 3407  # random seed for training
shuffle: yes  # whether shuffle order of training samples
train_batch_size: 8
grad_acc_steps: 1
eval_every_steps: 20  # evaluation after weight update steps
test_batch_size: 32
warmup_ratio: 0.05
weight_decay: 0.01
learning_rate: 2.0e-5
adam_epsilon: 1.0e-8
max_grad_norm: 1.0
max_train_epochs: 40
#!!!! 
# 20: LWP
# 40: BadNets and AddSent 

early_stop_steps: 5
save_metric: 'accuracy'
num_trial: 50
is_test: no  # only test or evaluate using all the clean/poisoning data

