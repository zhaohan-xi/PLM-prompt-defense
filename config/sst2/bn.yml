# Data
task_name: SST2

model_path: ./OpenBackdoor/ob-poison/model/sst-2/bn/last.ckpt
poison_path: ./OpenBackdoor/ob-poison/data/bn/sst-2.tsv 

# model_path: ./OpenBackdoor/ob-poison/sst-2/roberta-large-bn-False/0.1-2-1/last.ckpt
# poison_path: ./OpenBackdoor/ob-poison/sst-2/roberta-large-bn-False/0.1-2-1/poisoned.tsv

# model_path: ./OpenBackdoor/ob-poison/sst-2/roberta-large-bn-False/0.1-3-1/last.ckpt
# poison_path: ./OpenBackdoor/ob-poison/sst-2/roberta-large-bn-False/0.1-3-1/poisoned.tsv

# model_path: ./OpenBackdoor/ob-poison/sst-2/roberta-large-bn-False/0.2-1-1/last.ckpt
# poison_path: ./OpenBackdoor/ob-poison/sst-2/roberta-large-bn-False/0.2-1-1/poisoned.tsv

# model_path: ./OpenBackdoor/ob-poison/sst-2/roberta-large-bn-False/0.3-1-1/last.ckpt
# poison_path: ./OpenBackdoor/ob-poison/sst-2/roberta-large-bn-False/0.3-1-1/poisoned.tsv

# model_path: ./OpenBackdoor/ob-poison/sst-2/bert-large-uncased-bn-False/0.1-1-1/last.ckpt
# poison_path: ./OpenBackdoor/ob-poison/sst-2/bert-large-uncased-bn-False/0.1-1-1/poisoned.tsv

# model_path: ./OpenBackdoor/ob-poison/sst-2/bert-large-cased-bn-False/0.1-1-1/last.ckpt
# poison_path: ./OpenBackdoor/ob-poison/sst-2/bert-large-cased-bn-False/0.1-1-1/poisoned.tsv

# model_path: ./OpenBackdoor/ob-poison/sst-2/facebook/bart-large-bn-False/0.1-1-1/last.ckpt
# poison_path: ./OpenBackdoor/ob-poison/sst-2/facebook/bart-large-bn-False/0.1-1-1/poisoned.tsv

# model_path: ./OpenBackdoor/ob-poison/sst-2/gpt2-bn-False/0.1-1-1/last.ckpt
# poison_path: ./OpenBackdoor/ob-poison/sst-2/gpt2-bn-False/0.1-1-1/poisoned.tsv

# model_path: ./OpenBackdoor/ob-poison/sst-2/roberta-large-bn-True/0.1-1-1/last.ckpt
# poison_path: ./OpenBackdoor/ob-poison/sst-2/roberta-large-bn-True/0.1-1-1/poisoned.tsv  

# model_path: /data/zhaohan/MDP/OpenBackdoor/ob-poison/sst-2/roberta-large-bn-True/0.1-[3, 5]-1/last.ckpt
# poison_path: ./OpenBackdoor/ob-poison/sst-2/roberta-large-bn-True/0.1-[3, 5]-1/poisoned.tsv 

# model_path: /data/zhaohan/MDP/OpenBackdoor/ob-poison/sst-2/roberta-large-bn-True/0.1--1-1/last.ckpt
# poison_path: ./OpenBackdoor/ob-poison/sst-2/roberta-large-bn-True/0.1--1-1/poisoned.tsv 

# model_path: /data/zhaohan/MDP/OpenBackdoor/ob-poison/sst-2/roberta-large-bn-True/0.1--2-1/last.ckpt
# poison_path: ./OpenBackdoor/ob-poison/sst-2/roberta-large-bn-True/0.1--2-1/poisoned.tsv 

train_path: data/k-shot/SST-2/16-13/train.tsv
dev_path: data/k-shot/SST-2/16-13/dev.tsv
test_path: data/k-shot/SST-2/16-13/test.tsv

# styleattack
# model_path: /data/zhaohan/StyleAttack/experiments/save/sst-2-roberta-large.ckpt
# poison_path: /data/zhaohan/StyleAttack/data/transfer/bible/sst-2/test.tsv
# test_path: /data/zhaohan/StyleAttack/data/clean/sst-2/test.tsv

# Pretrained model path or name
pretrain_model: roberta-large

# Output
output_dir: output/bn/sst2/
log_file: sst2.log
pred_file: sst2.npy  # save predicted label ids

# Prompt
pet_method: diffpet # pet, diffpet
full_vocab_loss: yes
mask_rate: 0.2  # set 0.0 to disable auxiliary MLM loss
detect_mask_rate: 0.2
# 0.25: LWP
# 0.2: AddSent, BadNets
#
#
pred_loss_weight: 1  # weight of prediction loss
# !!!!
# 1: BadNets, AddSent
# 0: LWP

mlm_loss_weight: 0.1  # weight of auxiliary MLM loss
# !!!!
# 0.1: AddSent, BadNets
# 0.5: LWP

# encoder_type: none  # emb, mlp, lstm, none

# Train & evaluation
use_gpu: yes
max_seq_len: 128
seed: 3407  # random seed for training
shuffle: yes  # whether shuffle order of training samples
train_batch_size: 8
grad_acc_steps: 1
eval_every_steps: 20  # evaluation after weight update steps
test_batch_size: 32
warmup_ratio: 0.05
weight_decay: 0.01
learning_rate: 2.0e-5
adam_epsilon: 1.0e-8
max_grad_norm: 1.0
max_train_epochs: 40
#!!!! 
# 20: LWP
# 40: BadNets and AddSent 

early_stop_steps: 5
save_metric: 'accuracy'
num_trial: 50
is_test: no  # only test or evaluate using all the clean/poisoning data

